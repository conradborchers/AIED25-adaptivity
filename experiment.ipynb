{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llm_recommendation_proxy.router import Router\n",
    "from pprint import pp\n",
    "from copy import deepcopy\n",
    "from openai import OpenAI\n",
    "from functools import cache\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1843d0fe5611be3",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load example data\n",
    "\n",
    "We use synthetic data to illustrate the data used in this study. In the below code chunk, `requests` is a list containing information as dictionaries like:\n",
    "\n",
    "```\n",
    "requests[i] = {'tutor_id': 'math-parent-tool',\n",
    "  'KC': ['subtraction-const', 'cancel-var'],\n",
    "  'chat_history': ['Student: I did this wrong. I am not sure where i messed up',\n",
    "                   'Caregiver: ...',\n",
    "                   ... ] # more multiturn conversation between student and caregiver\n",
    "  'curr_question': '4x-10=-x+10',\n",
    "  'correct_step_history': '5x-10 = 10',\n",
    "  'incorrect_step_history': '3x-10 = 10',\n",
    "  'hints': [],\n",
    "  'next_steps': ['5x = 20', 'x = 4']},\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e10eb5aa83c07691"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_jsonl(file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data_list.append(json.loads(line.strip()))  # Remove leading/trailing spaces and parse each line as JSON\n",
    "    return data_list\n",
    "\n",
    "def dump_to_jsonl(data_list, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in data_list:\n",
    "            file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "requests = read_jsonl('test-data/mock_requests.json')\n",
    "pp(requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code chunk below are utility functions used for tutoring training classifier proposed by Thomas et al. (Danielle Thomas, Xinyu Yang, Shivang Gupta, Adetunji Adeniran, Elizabeth Mclaughlin, and Kenneth Koedinger. 2023. When the Tutor Becomes the Student: Design and Evaluation of Efficient Scenario-based Lessons for Tutors. In LAK23: 13th International Learning Analytics and Knowledge Conference (LAK2023). Association for Computing Machinery, New York, NY, USA, 250–261. https://doi.org/10.1145/3576050.3576089).\n",
    "\n",
    "The scoring prompts below are adapted from work by Thomas et al. (Thomas, D. R., Borchers, C., Kakarla, S., Lin, J., Bhushan, S., Guo, B., Gatz, E., & Koedinger, K. R. (2024). Do tutors learn from equity training and can generative AI assess it? [Preprint]. arXiv. https://doi.org/10.48550/arXiv.2412.11255)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a29e390877ba4c1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def rateErrorReactions(recs: list[str]):\n",
    "    SCORING_PROMPT_START = \\\n",
    "\"\"\"\n",
    "You are a tutor evaluator. Please score the following tutor response to a tutor training scenario involving a middle school student as follows:\n",
    "\n",
    "-If the tutor’s response provides an implicit way of calling attention to the error, focus on encouraging students thinking about math and  metacognition rather than giving explicit answers, and use effective praise to motivate students' learning, score with a 1. Examples of responses scoring a 1 are: “Lucy, great start! Explain how you passed to the second column.”; “Thank you for showing me your work, Lucy. You worked hard on adding these numbers. I am going to work on this problem with you. Can you tell me how you added the numbers first?”; “Well done setting up the problem, Lucy. Can you tell me how you went about calculating the first few steps?”; “I like your effort, but show me what you are trying to do in the second step.”; “Kanye, very well done. Your effort was very valuable. Can you repeat how you arrived at this result? Let's do this problem together.”\n",
    "\n",
    "-If the tutor's response directly addresses the student’s error using explicit language, or directly calls attention to the student’s error, or fails to encourage the students think about math or metacognition by not guiding them to reflect on their thought process, score with a 0.\n",
    "Sample responses scoring a 0 include: “Let's try solving the problem together.”; “Good effort.”; “I got a different answer. Let's look at this together.”; “Do you know about carrying the 1?”; “Kanye, let me show you how this should be done.”\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "    SCORING_FORMAT_PROMPT = \\\n",
    "\"\"\"\n",
    "--- Response End. Given the earlier response, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}.\n",
    "\"\"\"\n",
    "\n",
    "    scores = []\n",
    "    for rec in recs:\n",
    "        overall_history = [{\"role\": \"system\", \"content\": SCORING_PROMPT_START}, {\"role\": \"user\", \"content\": rec}, {\"role\": \"system\", \"content\": SCORING_FORMAT_PROMPT}]\n",
    "        openai_out = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=overall_history, max_tokens=300, temperature=0)\n",
    "        response_data = json.loads(openai_out.choices[0].message.content)\n",
    "        score = response_data[\"Score\"]\n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def ratePraises(recs: list[str]):\n",
    "    SCORING_PROMPT_START = \\\n",
    "\"\"\"\n",
    "You are a tutor evaluator. Please score the following tutor response to a tutor training scenario involving a middle school student as follows:\n",
    "\n",
    "-if the tutor’s response provides effective, process-focused praise that acknowledges the student’s effort, hard work, perseverance, or focuses on the student’s actions towards the learning process, score with a 1. Examples of responses scoring a 1 are: “Kevin, you didn't give up and you managed to learn, congratulations! Let's finish your math homework together so you can still get a good grade and learn how this kind of homework will be easier sooner.”; “Keep Working.”; “Great job Kevin! You are on the right track, keep working on the problem, you get it!”; “Kevin, that was awesome the way you kept at it and were able to get to the correct answer. You should be proud. Keep up the great work!”; “You're doing a great job working on this paragraph! It can be tricky to find the right words and I think you're doing really well working through it.”\n",
    "\n",
    "-if the tutor's response provides outcomes-based praise, acknowledging only the student’s achievements or outcomes, or does not acknowledge the learning process or effort towards learning, score with a 0. Sample responses scoring a 0 include: “You\\'re doing great, let\\'s see what the next step is.”; “Good Job.”; “I would say she is doing well and let us explore a bit more.”; “I think you are doing great.”; “You can do this! Just take it one step at a time.”\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "    SCORING_FORMAT_PROMPT = \\\n",
    "\"\"\"\n",
    "--- Response End. Given the earlier response, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}.\n",
    "\"\"\"\n",
    "    scores = []\n",
    "    for rec in recs:\n",
    "        overall_history = [{\"role\": \"system\", \"content\": SCORING_PROMPT_START}, {\"role\": \"user\", \"content\": rec}, {\"role\": \"system\", \"content\": SCORING_FORMAT_PROMPT}]\n",
    "        openai_out = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=overall_history, max_tokens=300, temperature=0)\n",
    "        response_data = json.loads(openai_out.choices[0].message.content)\n",
    "        score = response_data[\"Score\"]\n",
    "        scores.append(score)\n",
    "        \n",
    "    return scores\n",
    "\n",
    "def evalResponses(requests: list[dict], responses: list[str], prompts: list[str]) -> list[dict]:\n",
    "    evalResult = list()\n",
    "    for request, response, prompt in zip(requests, responses, prompts):\n",
    "        entry = dict()\n",
    "        entry['response'] = response\n",
    "        entry['prompt'] = prompt\n",
    "        entry['has_intention'] = '[' in response and ']' in response\n",
    "        entry['has_delimiter'] = '#' in response\n",
    "        entry['has_three_recommendations'] = entry['has_delimiter'] and len(response.split('#')) == 3\n",
    "        entry['has_no_prefix'] = response.strip().startswith('[')\n",
    "        if not request['incorrect_step_history']:\n",
    "            entry['praise_ratings'] = ratePraises(response.split('#'))\n",
    "        else:\n",
    "            entry['error_reaction_ratings'] = rateErrorReactions(response.split('#'))\n",
    "        \n",
    "        evalResult.append(entry)\n",
    "        \n",
    "    report = dict()\n",
    "    for entry in evalResult:\n",
    "        for key in entry.keys():\n",
    "            if key not in report: report[key] = [0, 0]\n",
    "            if isinstance(entry[key], bool):\n",
    "                report[key][0] += int(entry[key])\n",
    "                report[key][1] += 1\n",
    "            elif 'ratings' in key:\n",
    "                report[key][0] += sum(entry[key])\n",
    "                report[key][1] += len(entry[key])\n",
    "    print(\"*** Recommendation quality evaluation report: ***\")\n",
    "    pp(report)\n",
    "    \n",
    "    return evalResult\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "335beefe2478bcde",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the three code chunks below, we prompt the three models with the prompts generated from previously loaded context examples. These responses are then rated by the tutor training classifier, leading to the results reported in Table 2 in the manuscript. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4194865acd68ce4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# GPT-4o responses for raw requests\n",
    "router = Router()\n",
    "responses = []\n",
    "prompts = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"GPT-4o\"\n",
    "    response, prompt = router.processRequest(request, mode=\"test\")\n",
    "    responses.append(response)\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "dump_to_jsonl(evalResponses(requests, responses, prompts), 'test-data/gpt-4o-responses.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b956c6336529119b",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Llama3-8B responses for raw requests\n",
    "router = Router()\n",
    "responses = []\n",
    "prompts = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"Llama3-8B\"\n",
    "    response, prompt = router.processRequest(request, mode=\"test\")\n",
    "    responses.append(response)\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "dump_to_jsonl(evalResponses(requests, responses, prompts), 'test-data/llama3-8b-responses.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e7e82b2f75e716",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Llama3-70B responses for raw requests\n",
    "router = Router()\n",
    "responses = []\n",
    "prompts = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"Llama3-70B\"\n",
    "    response, prompt = router.processRequest(request, mode=\"test\")\n",
    "    responses.append(response)\n",
    "    prompts.append(prompt)\n",
    "    \n",
    "dump_to_jsonl(evalResponses(requests, responses, prompts), 'test-data/llama3-70b-responses.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1de6f775b7484834",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "GPT-4o quality report:\n",
    "*** Recommendation quality evaluation report: ***\n",
    "{'has_intention': [75, 75],\n",
    " 'has_delimiter': [75, 75],\n",
    " 'has_three_recommendations': [75, 75],\n",
    " 'has_no_prefix': [75, 75],\n",
    " 'praise_ratings': [102, 153],\n",
    " 'error_reaction_ratings': [40, 72]}\n",
    " \n",
    "Llama3-8B quality report:\n",
    "*** Recommendation quality evaluation report: ***\n",
    "{'has_intention': [71, 75],\n",
    " 'has_delimiter': [31, 75],\n",
    " 'has_three_recommendations': [26, 75],\n",
    " 'has_no_prefix': [64, 75],\n",
    " 'praise_ratings': [75, 94],\n",
    " 'error_reaction_ratings': [28, 40]}\n",
    "\n",
    "Llama3-70B quality report:\n",
    "*** Recommendation quality evaluation report: ***\n",
    "{'has_intention': [73, 75],\n",
    " 'has_delimiter': [73, 75],\n",
    " 'has_three_recommendations': [72, 75],\n",
    " 'has_no_prefix': [72, 75],\n",
    " 'praise_ratings': [104, 150],\n",
    " 'error_reaction_ratings': [34, 72]}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acb94714e336b907"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate responsiveness\n",
    "\n",
    "We then proceed to analysis on LLMs' responsiveness typical to ITS as described in sections 2.5 and 2.6. To begin with, we first define the routine to conduct context variation:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fdd9673e4ac8ba"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def variateRequest(request: dict) -> tuple[list[str], list[dict]]:\n",
    "    variants = [request]\n",
    "    \n",
    "    reqWithoutCorrectSteps = deepcopy(request)\n",
    "    reqWithoutCorrectSteps[\"correct_step_history\"] = []\n",
    "    variants.append(reqWithoutCorrectSteps)\n",
    "    \n",
    "    reqWithoutIncorrectSteps = deepcopy(request)\n",
    "    reqWithoutIncorrectSteps[\"incorrect_step_history\"] = []\n",
    "    variants.append(reqWithoutIncorrectSteps)\n",
    "    \n",
    "    reqWithoutNextSteps = deepcopy(request)\n",
    "    reqWithoutNextSteps[\"next_steps\"] = []\n",
    "    variants.append(reqWithoutNextSteps)\n",
    "    \n",
    "    reqWithoutKC = deepcopy(request)\n",
    "    reqWithoutKC[\"KC\"] = []\n",
    "    variants.append(reqWithoutKC)\n",
    "    \n",
    "    reqWithoutHints = deepcopy(request)\n",
    "    reqWithoutHints[\"hints\"] = []\n",
    "    variants.append(reqWithoutHints)\n",
    "    \n",
    "    return [\"raw\", \"lack_correct_steps\", \"lack_incorrect_steps\", \"lack_next_steps\", \"lack_kc\", \"lack_hints\"], variants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66af08ccb00414ae",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then prompt the three models with contexts after variation. These generation are then saved to local json files for further analysis. These files are also manually examined to produce results presented in section 3.3."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44c20f572a3430e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# GPT-4o responses for responsiveness tester requests\n",
    "router = Router()\n",
    "responseGroups = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"GPT-4o\"\n",
    "    variantTypes, variants = variateRequest(request)\n",
    "    group = dict()\n",
    "    for vType, v in zip(variantTypes, variants):\n",
    "        response = router.processRequest(v, mode=\"test\")[0]\n",
    "        group[vType] = response\n",
    "    \n",
    "    responseGroups.append(group)\n",
    "    \n",
    "dump_to_jsonl(responseGroups, 'test-data/gpt-4o-responsiveness-v2.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c8a318287df0d0",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Llama3-8B responses for responsiveness tester requests\n",
    "router = Router()\n",
    "responseGroups = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"Llama3-8B\"\n",
    "    variantTypes, variants = variateRequest(request)\n",
    "    group = dict()\n",
    "    for vType, v in zip(variantTypes, variants):\n",
    "        response = router.processRequest(v, mode=\"test\")[0]\n",
    "        group[vType] = response\n",
    "    \n",
    "    responseGroups.append(group)\n",
    "    \n",
    "dump_to_jsonl(responseGroups, 'test-data/llama3-8b-responsiveness-v2.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66971aa394d04221",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Llama3-70B responses for responsiveness tester requests\n",
    "router = Router()\n",
    "responseGroups = []\n",
    "for request in requests:\n",
    "    request = deepcopy(request)\n",
    "    request[\"model_id\"] = \"Llama3-70B\"\n",
    "    variantTypes, variants = variateRequest(request)\n",
    "    group = dict()\n",
    "    for vType, v in zip(variantTypes, variants):\n",
    "        response = router.processRequest(v, mode=\"test\")[0]\n",
    "        group[vType] = response\n",
    "    \n",
    "    responseGroups.append(group)\n",
    "    \n",
    "dump_to_jsonl(responseGroups, 'test-data/llama3-70b-responsiveness-v2.jsonl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d1fcde6579f0f55",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have obtained LLM generations for variant contexts, we define the following helper routines to obtain sentence embedding and conduct statistical testing (section 2.5). Notice that the main mathematical procedure described in section 2.5 can be found in the `permutation_test()` function below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d009ec42a9839a50"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "@cache\n",
    "def getEmbedding(text, model=\"text-embedding-3-large\"):\n",
    "    return np.array(client.embeddings.create(input=text, model=model, encoding_format=\"float\").data[0].embedding)\n",
    "\n",
    "def assessCosSimilarity(matrix1, matrix2):\n",
    "    dot_product = np.sum(matrix1 * matrix2, axis=1)\n",
    "    norm1 = np.linalg.norm(matrix1, axis=1)\n",
    "    norm2 = np.linalg.norm(matrix2, axis=1)\n",
    "    cosine_similarities = dot_product / (norm1 * norm2)\n",
    "    return cosine_similarities\n",
    "\n",
    "def assessEuclideanDistance(matrix1, matrix2):\n",
    "    distances = np.sum((matrix1 - matrix2) ** 2, axis=1) ** 0.5\n",
    "    return distances\n",
    "    \n",
    "def permutation_test(matrix1, matrix2, n_permutations=1000, metric='euclidean', seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    combined = np.vstack((matrix1, matrix2))\n",
    "    obs_stat = np.mean(cdist(matrix1, matrix2, metric=metric))\n",
    "    \n",
    "    perm_stats = []\n",
    "\n",
    "    for _ in range(n_permutations):\n",
    "        np.random.shuffle(combined)\n",
    "        perm_group1 = combined[:len(matrix1)]\n",
    "        perm_group2 = combined[len(matrix1):]\n",
    "        \n",
    "        perm_stat = np.mean(cdist(perm_group1, perm_group2, metric=metric))\n",
    "        perm_stats.append(perm_stat)\n",
    "\n",
    "    perm_stats = np.array(perm_stats)\n",
    "    p_value = np.mean(perm_stats >= obs_stat)\n",
    "    effect_size = (obs_stat - np.mean(perm_stats)) / np.std(perm_stats)\n",
    "    \n",
    "    print(\"Permutation test summary:\")\n",
    "    print(f\"p-value = {p_value}\")\n",
    "    print(f\"effect size = {effect_size}\")\n",
    "    print(f\"Observed test statistic = {obs_stat}\")\n",
    "    \n",
    "def permutation_test_report(embeddingGroups: dict, group1: str, group2: str, metric: str):\n",
    "    print(f\"*** Permutation test report between {group1} and {group2}; metric = {metric} ***\")\n",
    "    permutation_test(embeddingGroups[group1], embeddingGroups[group2], metric=metric, seed=42)\n",
    "\n",
    "def assessResponsiveness(jsonlFilePath: str):\n",
    "    data = read_jsonl(jsonlFilePath)\n",
    "    embeddingGroups = dict()\n",
    "    for line in data:\n",
    "        for key in line:\n",
    "            if key not in embeddingGroups:\n",
    "                embeddingGroups[key] = getEmbedding(line[key])\n",
    "            else:\n",
    "                embeddingGroups[key] = np.vstack((embeddingGroups[key], getEmbedding(line[key])))\n",
    "                \n",
    "    keys = set(embeddingGroups.keys())\n",
    "    keys.remove(\"raw\")\n",
    "    for k in keys:\n",
    "        cosineSimilarity = assessCosSimilarity(embeddingGroups['raw'], embeddingGroups[k])\n",
    "        eucDistance = assessEuclideanDistance(embeddingGroups['raw'], embeddingGroups[k])\n",
    "        print(f\"Average cosine similarity between original responses and {k} (mean, std): {cosineSimilarity.mean(), cosineSimilarity.std()}\")\n",
    "        print(f\"Average euclidean distance between original responses and {k} (mean, std): \"\n",
    "              f\"{cosineSimilarity.mean(), cosineSimilarity.std()}\")\n",
    "    \n",
    "    return embeddingGroups\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d037c5148ec551da",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We obtain the embedding vector through the `assessResponsiveness` routine, and conduct statistical testing to obtain the results in table 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfbedb4399a9a3fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddingGroups = assessResponsiveness(\"test-data/gpt-4o-responsiveness-v2.jsonl\")\n",
    "with open(\"test-data/gpt-4o-embeddings-v2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(embeddingGroups, file)\n",
    "\n",
    "with open(\"test-data/gpt-4o-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    embeddingGroups = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db520d1a1b0dd619",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_incorrect_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_correct_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_next_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_hints\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_kc\", \"cosine\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a15444febc14e1f",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "*** Permutation test report between raw and lack_incorrect_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.293\n",
    "effect size = 0.33376539136559175\n",
    "Observed test statistic = 0.36230536893481025\n",
    "\n",
    "*** Permutation test report between raw and lack_correct_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.995\n",
    "effect size = -1.6586528530972622\n",
    "Observed test statistic = 0.345359725751045\n",
    "\n",
    "*** Permutation test report between raw and lack_next_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.999\n",
    "effect size = -1.675421824371236\n",
    "Observed test statistic = 0.3508862006414789\n",
    "\n",
    "*** Permutation test report between raw and lack_hints; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 1.0\n",
    "effect size = -2.164320111282954\n",
    "Observed test statistic = 0.34896716730660776\n",
    "\n",
    "*** Permutation test report between raw and lack_kc; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.998\n",
    "effect size = -1.902855526566746\n",
    "Observed test statistic = 0.35479692058624535"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "310b380df8efcf4c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddingGroups = assessResponsiveness(\"test-data/llama3-8b-responsiveness-v2.jsonl\")\n",
    "with open(\"test-data/llama3-8b-embeddings-v2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(embeddingGroups, file)\n",
    "    \n",
    "with open(\"test-data/llama3-8b-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    embeddingGroups = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5dc7ceb76596d",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_incorrect_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_correct_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_next_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_hints\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_kc\", \"cosine\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a4fa2312e34a930",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "*** Permutation test report between raw and lack_incorrect_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.904\n",
    "effect size = -1.207121894183875\n",
    "Observed test statistic = 0.40457623818175104\n",
    "\n",
    "*** Permutation test report between raw and lack_correct_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.997\n",
    "effect size = -1.861878543986267\n",
    "Observed test statistic = 0.4163100891698207\n",
    "\n",
    "*** Permutation test report between raw and lack_next_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.775\n",
    "effect size = -0.7493554854857315\n",
    "Observed test statistic = 0.40868814795318426\n",
    "\n",
    "*** Permutation test report between raw and lack_hints; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 1.0\n",
    "effect size = -1.969807363686397\n",
    "Observed test statistic = 0.390900029054426\n",
    "\n",
    "*** Permutation test report between raw and lack_kc; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.998\n",
    "effect size = -1.9957248032178274\n",
    "Observed test statistic = 0.40768793691835165"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77e4ad847b389680"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embeddingGroups = assessResponsiveness(\"test-data/llama3-70b-responsiveness-v2.jsonl\")\n",
    "with open(\"test-data/llama3-70b-embeddings-v2.pkl\", \"wb\") as file:\n",
    "    pickle.dump(embeddingGroups, file)\n",
    "    \n",
    "with open(\"test-data/llama3-70b-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    embeddingGroups = pickle.load(file)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e4d6626417e3bc7",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_incorrect_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_correct_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_next_steps\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_hints\", \"cosine\")\n",
    "permutation_test_report(embeddingGroups, \"raw\", \"lack_kc\", \"cosine\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d685f11a7275db",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "*** Permutation test report between raw and lack_incorrect_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.035\n",
    "effect size = 2.362433524312577\n",
    "Observed test statistic = 0.37279672420121496\n",
    "\n",
    "*** Permutation test report between raw and lack_correct_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.304\n",
    "effect size = 0.19052223036696725\n",
    "Observed test statistic = 0.35355661589904624\n",
    "\n",
    "*** Permutation test report between raw and lack_next_steps; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.997\n",
    "effect size = -1.392041635541663\n",
    "Observed test statistic = 0.36636623574767907\n",
    "\n",
    "*** Permutation test report between raw and lack_hints; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 1.0\n",
    "effect size = -1.8790553915040875\n",
    "Observed test statistic = 0.3603393836893666\n",
    "\n",
    "*** Permutation test report between raw and lack_kc; metric = cosine ***\n",
    "Permutation test summary:\n",
    "p-value = 0.994\n",
    "effect size = -1.374257712690145\n",
    "Observed test statistic = 0.35429153463643104"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be5a8d1ba7683d22"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering analysis on response embedding\n",
    "\n",
    "This portion of notebook corresponds to section 2.6 and 3.3, where can conduct PCA and discover clustering difference between the three models. For the code block below, can read back the embeddings saved in files and concatenate them into one single numpy array."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1187137012734f89"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "with open(\"test-data/llama3-8b-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    llama_8b_embedding_groups = pickle.load(file)\n",
    "with open(\"test-data/llama3-70b-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    llama_70b_embedding_groups = pickle.load(file)\n",
    "with open(\"test-data/gpt-4o-embeddings-v2.pkl\", \"rb\") as file:\n",
    "    gpt_4o_embedding_groups = pickle.load(file)\n",
    "    \n",
    "model_names = []\n",
    "class_names = []\n",
    "all_embeddings = None\n",
    "for model_name, embedding_group in [(\"llama3-8b\", llama_8b_embedding_groups),\n",
    "                                    (\"llama3-70b\", llama_70b_embedding_groups),\n",
    "                                    (\"gpt-4o\", gpt_4o_embedding_groups)]:\n",
    "    for class_name, embeddings in embedding_group.items():\n",
    "        n_samples = embeddings.shape[0]\n",
    "        model_names.extend([model_name for _ in range(n_samples)])\n",
    "        class_names.extend([class_name for _ in range(n_samples)])\n",
    "        if all_embeddings is None:\n",
    "            all_embeddings = embeddings\n",
    "        else:\n",
    "            all_embeddings = np.concatenate((all_embeddings, embeddings), axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec1d5a9759f35f78",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then perform PCA to reduce them into 2D"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f488fe224f0dbad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(all_embeddings)\n",
    "explained_variance_retained = sum(pca.explained_variance_ratio_)\n",
    "variance_lost = 1 - explained_variance_retained\n",
    "print(f\"Proportion of variance lost in PCA: {variance_lost}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25a031a7e75a5209",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "Proportion of variance lost in PCA: 0.7506287335061888"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6845ce819749668d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We scatter these 2D data points into the diagram below."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffbc82ad8aa5136c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# create a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], alpha=0.7, s=10)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA-Reduced Embeddings in 2D')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "plt.axvline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6243c5857ff5258e",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "To be able to discover the clustering behavior of the LLMs' embedding vectors, we color these data points by their original model and visualize each groups variance in 2D via the eclipse of the same color."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39eecbde6aa038c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "eclipse_colors = [\"tab:blue\", \"orange\", \"green\"]\n",
    "for i, label in enumerate((\"Llama3-8B\", \"Llama3-70B\", \"GPT-4o\")):\n",
    "    slice_start = int(i * reduced_data.shape[0] / 3)\n",
    "    slice_end = int(slice_start + reduced_data.shape[0] / 3)\n",
    "    data = reduced_data[slice_start:slice_end, :]\n",
    "\n",
    "    # Scatter the data points\n",
    "    plt.scatter(\n",
    "        data[:, 0], \n",
    "        data[:, 1], \n",
    "        label=label,  # Add label for the legend\n",
    "        alpha=0.8, \n",
    "        s=10\n",
    "    )\n",
    "    \n",
    "    # Calculate the mean and covariance matrix of the group\n",
    "    mean = np.mean(data, axis=0)\n",
    "    cov = np.cov(data, rowvar=False)\n",
    "\n",
    "    # Generate an ellipse based on the Mahalanobis distance\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    order = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[order]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "\n",
    "    # Calculate the angle of the ellipse\n",
    "    angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
    "\n",
    "    # Scaling for the ellipse (e.g., 95% confidence interval)\n",
    "    chisq_val = 5.991  # 95% confidence for 2D\n",
    "    width = 2 * np.sqrt(chisq_val * eigenvalues[0])\n",
    "    height = 2 * np.sqrt(chisq_val * eigenvalues[1])\n",
    "\n",
    "    # Add ellipse to the plot\n",
    "    ellipse = Ellipse(\n",
    "        xy=mean,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        angle=angle,\n",
    "        edgecolor=eclipse_colors[i],\n",
    "        facecolor='none',\n",
    "        linestyle='--',\n",
    "        linewidth=1.5,\n",
    "        label=f'{label} (Covariance Ellipse)'\n",
    "    )\n",
    "    plt.gca().add_patch(ellipse)\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "plt.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "plt.axvline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e65c4e240da5db2",
   "execution_count": 0
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7cde83db345a69cb",
   "execution_count": 0
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
